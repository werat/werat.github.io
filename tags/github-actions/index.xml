<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>github-actions on Reboot and Shine</title><link>https://werat.dev/tags/github-actions/</link><description>Recent content in github-actions on Reboot and Shine</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Thu, 13 May 2021 13:35:00 +0100</lastBuildDate><atom:link href="https://werat.dev/tags/github-actions/index.xml" rel="self" type="application/rss+xml"/><item><title>Running benchmarks for Pull Requests via GitHub Actions</title><link>https://werat.dev/blog/running-benchmarks-for-pull-requests-via-github-actions/</link><pubDate>Thu, 13 May 2021 13:35:00 +0100</pubDate><guid>https://werat.dev/blog/running-benchmarks-for-pull-requests-via-github-actions/</guid><description>Benchmarks are often underestimated and don&amp;rsquo;t get the same attention as tests. However &amp;ldquo;performance is a feature&amp;rdquo; and when something is not tested it might as well be just broken. If the performance is not measured/tracked regressions are inevitable.
Modern tooling makes it really easy to write benchmarks. Some languages have built-in support, for example, Rust comes with cargo bench (docs) and Go has go test -bench (docs). For C++ there is google/benchmark &amp;ndash; not as streamlined as having it built into the language infrastructure, but still definitely worth the effort.</description></item></channel></rss>